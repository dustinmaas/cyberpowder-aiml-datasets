{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae4792f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# O-RAN AI/ML Model Development, Training, and Export\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "- Completion of the O-RAN Datasets and Analysis Session and uploading your\n",
    "  processed dataset to HuggingFace\n",
    "- Reading chapters 1 and 2 from [Understanding Deep Learning by Simon\n",
    "  Prince](https://udlbook.github.io/udlbook/) (optional if you are already\n",
    "  familiar with ML concepts). The other resources on the book website are also\n",
    "  quite useful.\n",
    "- Reading this [PyTorch tutorial on Linear\n",
    "  Regression](https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_linear_regression/)\n",
    "  (optional if you are already somewhat familiar with PyTorch)\n",
    "- You might want to make a copy of this notebook to your own Google drive\n",
    "  account so you can save your work. You can do this by clicking on the \"Save a\n",
    "  copy in Drive\" in the File menu.\n",
    "- **Note: Don't just run the whole notebook. There are some cells that will\n",
    "  require interaction. Read the comments and understand what each cell does.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e36f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (various other required packages are already available in the colab environment)\n",
    "!uv -q pip install datasets onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b975b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "# Standard library imports\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import datasets\n",
    "import huggingface_hub as hf\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "\n",
    "# From-style imports\n",
    "from huggingface_hub import hf_hub_download\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8feee6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Add Your Token as a Secret in Colab, Log in to HuggingFace, and Download Your Processed Dataset\n",
    "\n",
    "### Walkthrough adding your token as a secret in Colab\n",
    "\n",
    "After adding the token as a secret in Colab and use the\n",
    "following code to get the token:\n",
    "\n",
    "```\n",
    "from google.colab import userdata\n",
    "userdata.get('hf_token')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a28191a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Log in to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d5113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import userdata\n",
    "token = userdata.get('hf_token')\n",
    "hf.login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6262c8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Check that you're logged in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401230cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = hf.whoami()['name']\n",
    "print(f\"Logged in as {username}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cd626",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Download your dataset\n",
    "\n",
    "First, let's list the datasets you have access to under your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f573ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_api = hf.HfApi()\n",
    "my_datasets = hf_api.list_datasets(author=username)\n",
    "for ds in my_datasets:\n",
    "    print(f\"Dataset: {ds.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d8786d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now download the processed dataset you uploaded in the previous session. You\n",
    "should have uploaded it to your personal HuggingFace account, not the\n",
    "CyberPowder Org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa82d60",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "dataset_name = \"cyberpowder-network-metrics\"  # Fill in the name of the processed dataset\n",
    "dataset_id = f\"{username}/{dataset_name}\"\n",
    "\n",
    "# Download the dataset\n",
    "dataset = datasets.load_dataset(dataset_id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b2d74",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Verify the dataset looks like you expect\n",
    "\n",
    "You can create a pandas DataFrame from the dataset, then you use the DataFrame\n",
    "to create a plots to inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b1fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "# change timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc65e6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Let's regenerate the plots from the previous session\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add each metric as a separate trace\n",
    "fig.add_trace(go.Scatter(x=df['timestamp'], y=df['atten'], mode='lines', name='atten'))\n",
    "fig.add_trace(go.Scatter(x=df['timestamp'], y=df['CQI'], mode='lines', name='CQI'))\n",
    "fig.add_trace(go.Scatter(x=df['timestamp'], y=df['RSRP'], mode='lines', name='RSRP'))\n",
    "fig.add_trace(go.Scatter(x=df['timestamp'], y=df['DRB.UEThpDl'] / 1000.0, mode='lines', name='DRB.UEThpDl (Mbps)'))\n",
    "fig.add_trace(go.Scatter(x=df['timestamp'], y=df['min_prb_ratio'], mode='lines', name='min_prb_ratio'))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Time Series of Network Metrics',\n",
    "    xaxis_title='Timestamp',\n",
    "    yaxis_title='Value',\n",
    "    legend_title='KPIs and Parameters',\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "# Add range slider\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        rangeslider=dict(visible=True),\n",
    "        type=\"date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a27030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scatter_for_prb(df, prb_value):\n",
    "    df_filtered = df[df['min_prb_ratio'] == prb_value]\n",
    "    return go.Scatter(\n",
    "        x=df_filtered['CQI'],\n",
    "        y=df_filtered['DRB.UEThpDl'] / 1000.0,  # Convert to Mbps\n",
    "        mode='markers',\n",
    "        name=f'min_prb_ratio = {prb_value}',\n",
    "        marker=dict(\n",
    "            size=8,\n",
    "            opacity=0.7,\n",
    "        ),\n",
    "        hovertemplate='CQI: %{x}<br>Throughput: %{y:.2f} Mbps<extra></extra>'\n",
    "    )\n",
    "\n",
    "# Get unique min_prb_ratio values\n",
    "unique_prb_values = sorted(df['min_prb_ratio'].unique())\n",
    "\n",
    "# We don't need plots for every min_prb_ratio value, so let's just take every fifth value\n",
    "unique_prb_values = unique_prb_values[::5]\n",
    "\n",
    "# Create subplot grid with one subplot per min_prb_ratio value\n",
    "fig = make_subplots(\n",
    "    rows=1, \n",
    "    cols=len(unique_prb_values),\n",
    "    subplot_titles=[f'min_prb_ratio = {val}' for val in unique_prb_values],\n",
    "    shared_yaxes=True\n",
    ")\n",
    "\n",
    "# Add a scatter trace for each min_prb_ratio value\n",
    "for i, prb_value in enumerate(unique_prb_values):\n",
    "    fig.add_trace(\n",
    "        make_scatter_for_prb(df, prb_value),\n",
    "        row=1, \n",
    "        col=i+1\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Throughput vs. CQI by min_prb_ratio',\n",
    "    height=500,\n",
    "    width=200 * len(unique_prb_values),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Update axes labels\n",
    "for i in range(len(unique_prb_values)):\n",
    "    fig.update_xaxes(title_text=\"CQI\", row=1, col=i+1)\n",
    "    if i == 0:  # Only add y-axis title to the first subplot\n",
    "        fig.update_yaxes(title_text=\"Throughput (Mbps)\", row=1, col=i+1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448b782a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "You generated these time series and scatter plots for your *cleaned* data as\n",
    "part of the HW for the last session. The plots you generate in your Colab\n",
    "notebook should match those.\n",
    "\n",
    "If the plots look as you expect, you can proceed to the next step. If not, maybe\n",
    "you are not loading your processed dataset.\n",
    "\n",
    "Once you have verified the dataset, you can proceed to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c93fdd",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Creating and Training a Simple Linear Regression Model\n",
    "\n",
    "Now we will create a simple linear regression model to predict the minimum PRB\n",
    "ratio (`min_prb_ratio`) required to a achieve a various downlink throughput\n",
    "requirements (`DRB.UEThpDl`) for the priority slice (and emergency responder\n",
    "device), under a variety of channel conditions (`CQI`).\n",
    "\n",
    "### Linear Regression Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb950a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model class which inherits from torch.nn.Module\n",
    "class LinearRegressionModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A simple linear regression model with batch normalization for predicting the\n",
    "    minimum PRB ratio required to achieve a given downlink throughput for the\n",
    "    priority slice under various channel conditions.\n",
    "\n",
    "    We include normalization and denormalization steps within the model, but\n",
    "    these steps could be taken as pre/post-processing steps outside of the\n",
    "    model. For this exercise, we include them within the model for simplicity of\n",
    "    deployment later.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(2, 1)  # two input features, one output feature\n",
    "        \n",
    "        \"\"\"\n",
    "        Apply batch normalization to input features. Normalizing input features\n",
    "        to have zero mean and unit variance is a common practice that can help\n",
    "        improve model training. In general, normalizing input features can help\n",
    "        the model converge faster and reduce the risk of getting stuck in local\n",
    "        minima.\n",
    "        \n",
    "        Here, we are applying batch normalization to the input features before\n",
    "        passing them through the linear layer. This also allows us to feed the\n",
    "        features directly to the model without extra preprocessing.\n",
    "        \"\"\"\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(2)\n",
    "        \n",
    "        \"\"\"\n",
    "        We also need to keep track of the mean and standard deviation of the\n",
    "        target variable (min_prb_ratio) during training. This is because we\n",
    "        normalize the target variable during training and need to apply the same\n",
    "        normalization during inference to get the correct predictions.\n",
    "        \n",
    "        For our application, we want to return the predictions in the original\n",
    "        scale of the target variable (min_prb_ratio). To do this, we need to\n",
    "        store the mean and standard deviation of the target variable during\n",
    "        training so we can use them to \"denormalize\" the predictions after\n",
    "        inference and return values in the original scale to the xApp that will\n",
    "        be using the model.\n",
    "        \"\"\"\n",
    "        self.register_buffer('y_mean', torch.zeros(1))\n",
    "        self.register_buffer('y_std', torch.ones(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward method defines the computation performed at every call of\n",
    "        the model. In this case, calculating the output from normalized input\n",
    "        features.\n",
    "        \"\"\"\n",
    "        x_normalized = self.batch_norm(x)\n",
    "        output = self.linear(x_normalized)\n",
    "        \n",
    "        \"\"\"\n",
    "        After training, we want to return the predictions in the original scale\n",
    "        of the target variable (min_prb_ratio). To do this, we need to\n",
    "        \"denormalize\" the predictions by applying the mean and standard\n",
    "        deviation of the target variable that we stored during training.\n",
    "        \"\"\"\n",
    "        if not self.training:\n",
    "            with torch.no_grad():\n",
    "                output = output * self.y_std + self.y_mean\n",
    "                \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc026a7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Data Preparation (features, targets, and tensors)\n",
    "\n",
    "Now that we have defined the model, we need to prepare the data for training. We\n",
    "will use the `CQI` and `DRB.UEThpDl` columns as input features and the\n",
    "`min_prb_ratio` column as the target variable. So, we need to extract these\n",
    "columns from the dataset and create tensors from them.\n",
    "\n",
    "But what is a tensor? Tensors are multi-dimensional arrays used to represent\n",
    "data in PyTorch. They are similar to NumPy arrays, but with additional features\n",
    "that make them suitable for deep learning tasks. E.g.:\n",
    "\n",
    "1. They can be used on both CPUs and GPUs.\n",
    "2. They support automatic differentiation (autograd) for gradient-based\n",
    "   optimization. I.e., they efficiently compute and track the gradients of a\n",
    "   loss function with respect to the model parameters across all layers of the\n",
    "   network. This is essential for training deep learning models using\n",
    "   backpropagation.\n",
    "3. They can be used to define and store neural network weights, biases, and\n",
    "   activations, as well as other variables that you define for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b94431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the input features and target variable\n",
    "X = torch.tensor(df[['CQI', 'DRB.UEThpDl']].values, dtype=torch.float32)\n",
    "y = torch.tensor(df['min_prb_ratio'].values, dtype=torch.float32)\n",
    "\n",
    "# Reshape the target variable to have a shape of (n_samples, 1)\n",
    "y = y.view(-1, 1)\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed243f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "Now that we have the data in the form of tensors, we can create an instance of\n",
    "the `LinearRegressionModel` class and train it on the data. We will use the mean\n",
    "squared error (MSE) loss function and stochastic gradient descent (SGD) as the\n",
    "optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f233ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the device: GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f906ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()\n",
    "model.y_mean = y.mean(dim=0, keepdim=True)\n",
    "model.y_std = y.std(dim=0, keepdim=True)\n",
    "model.to(device)\n",
    "X.to(device)\n",
    "y.to(device)\n",
    "\n",
    "\"\"\"\n",
    "We'll use the mean squared error (MSE) loss function, which calculates the\n",
    "average squared difference between the predicted and actual values.\n",
    "\n",
    "[documentation](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)\n",
    "\"\"\"\n",
    "criterion = torch.nn.MSELoss() # Mean Squared Error\n",
    "\n",
    "\"\"\"\n",
    "We will use stochastic gradient descent (SGD) as the optimizer. SGD updates the\n",
    "model parameters based on the gradients of the loss function with respect to the\n",
    "model parameters. The learning rate determines the step size at each iteration\n",
    "of the optimization process. A smaller learning rate means smaller updates to\n",
    "the model parameters, while a larger learning rate means larger updates.\n",
    "\n",
    "Choosing a learning rate that is too high can cause the model to diverge, while\n",
    "a learning rate that is too low can cause the model to converge too slowly. In\n",
    "practice, you may need to experiment with different learning rates to find the\n",
    "one that works best for your model and dataset.\n",
    "\n",
    "A common approach is to start with a small learning rate and gradually increase\n",
    "it until the training process starts to diverge. Then, you can decrease the\n",
    "learning rate to find a more stable value.\n",
    "\n",
    "Alternatively, you can use a learning rate scheduler that automatically adjusts\n",
    "the learning rate during training based on the model's performance. This can\n",
    "help improve the model's convergence and reduce the risk of overfitting. In this\n",
    "case, we will just ue a fixed learning rate of 0.05.\n",
    "\n",
    "[documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\n",
    "\"\"\"\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.05)\n",
    "\n",
    "\"\"\"\n",
    "We will train the model for `num_epochs` epochs. An epoch is one complete pass\n",
    "through the entire training dataset. The number of epochs is a hyperparameter\n",
    "that you can tune to improve the model's performance.\n",
    "\n",
    "In general, more epochs mean more training, but also means more risk of\n",
    "overfitting the model to the training data, which can occur when, e.g., the\n",
    "model learns the noise in the training data.\n",
    "\"\"\"\n",
    "num_epochs = 200\n",
    "\n",
    "\"\"\"\n",
    "Let's create an array to store the loss values for each epoch. This will\n",
    "allow us to visualize the convergence of the loss function during training.\n",
    "\"\"\"\n",
    "losses = np.zeros(num_epochs)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    \"\"\"\n",
    "    Compute predictions and loss for the current epoch. The loss is calculated\n",
    "    against the normalized target variable (min_prb_ratio).\n",
    "    \"\"\"\n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, (y - model.y_mean) / model.y_std)\n",
    "\n",
    "    # Store the loss value for this epoch\n",
    "    losses[epoch] = loss.item()\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    The loss.backward() method computes the gradients of the loss function with\n",
    "    respect to the model parameters. This is done using backpropagation, which\n",
    "    is an efficient algorithm for computing gradients in neural networks. These\n",
    "    gradients will be used by the optimizer to update the model parameters.\n",
    "    \"\"\"\n",
    "    loss.backward()\n",
    "    \n",
    "    \"\"\"\n",
    "    The optimizer.step() method updates the model parameters based on the\n",
    "    gradients computed during the backward pass. The optimizer.zero_grad()\n",
    "    method clears the gradients of all optimized tensors. This is important\n",
    "    because, by default, gradients accumulate in PyTorch. If we don't clear the\n",
    "    gradients, they will accumulate over multiple iterations, which can lead to\n",
    "    incorrect updates to the model parameters. (In some cases, accumulating\n",
    "    gradients can be useful, but with our relatively simple model, we don't need\n",
    "    to do that.)\n",
    "    \"\"\"\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Normalized Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc35de",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# plot the convergence of the loss function\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(num_epochs), y=losses, mode='lines', name='Loss'))\n",
    "fig.update_layout(title='Normalized Loss Convergence During Training', xaxis_title='Epoch', yaxis_title='Normalized Loss')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbacf6c7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Learned Parameters\n",
    "\n",
    "Now that the model is trained, let's look at the learned parameters. We can\n",
    "extract the learned weights and bias from the model and print them. These\n",
    "parameters define the hyperplane that the model has learned to fit the data. (We\n",
    "use the term \"hyperplane\" here because it generalizes to higher dimensional\n",
    "feature spaces.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1a3d6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Get the learned parameters\n",
    "learned_weights = model.linear.weight.data.cpu().numpy()\n",
    "learned_bias = model.linear.bias.data.cpu().numpy()\n",
    "\n",
    "# Print the learned hyperplane equation\n",
    "print(\"\\nLearned Hyperplane:\")\n",
    "print(f\"min_prb_ratio = {learned_weights[0][0]:.2f}*CQI + {learned_weights[0][1]:.2f}*DRB.UEThpDl + {learned_bias[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81d05c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Visual Validation\n",
    "\n",
    "Now let's do some simple visual validation of the model, using predictions for\n",
    "the required minimum slice resources (`min_prb_ratio`) required for various\n",
    "channel conditions (`CQI`) and downling throughput requirements (`DRB.UEThpDl`).\n",
    "\n",
    "We are essentially fitting a plane to the data points in the 3D space defined by\n",
    "the `CQI`, `DRB.UEThpDl`, and `min_prb_ratio` variables. Let's visualize the\n",
    "hyperplane fit and discuss for few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Get the learned parameters\n",
    "learned_weights = model.linear.weight.data.cpu().numpy()\n",
    "learned_bias = model.linear.bias.data.cpu().numpy()\n",
    "\n",
    "# Print the learned hyperplane equation (in normalized space)\n",
    "print(\"\\nLearned Hyperplane (in normalized space):\")\n",
    "print(f\"y_normalized = {learned_weights[0][0]:.2f}*x1_normalized + {learned_weights[0][1]:.2f}*x2_normalized + {learned_bias[0]:.2f}\")\n",
    "\n",
    "# Get feature normalization parameters from the batch normalization layer\n",
    "x_mean = model.batch_norm.running_mean.cpu().numpy().reshape(1, -1)\n",
    "x_std = torch.sqrt(model.batch_norm.running_var).cpu().numpy().reshape(1, -1)\n",
    "\n",
    "# Create normalized versions of features and targets using the model's normalization parameters\n",
    "features_normalized = (X.cpu().numpy() - x_mean) / x_std\n",
    "targets_normalized = (y.cpu().numpy() - model.y_mean.cpu().numpy()) / model.y_std.cpu().numpy()\n",
    "\n",
    "# Get original denormalized data\n",
    "features_denormalized = X.cpu().numpy()\n",
    "targets_denormalized = y.cpu().numpy()\n",
    "\n",
    "# Sample every 5th point for clarity in the scatter plot\n",
    "sample_indices = np.arange(0, len(features_denormalized), 5)\n",
    "features_sampled = features_denormalized[sample_indices]\n",
    "targets_sampled = targets_denormalized[sample_indices]\n",
    "\n",
    "# Create a meshgrid for the hyperplane using denormalized feature ranges\n",
    "x1_range = np.linspace(features_denormalized[:,0].min(), features_denormalized[:,0].max(), 20)\n",
    "x2_range = np.linspace(features_denormalized[:,1].min(), features_denormalized[:,1].max(), 20)\n",
    "X1, X2 = np.meshgrid(x1_range, x2_range)\n",
    "\n",
    "# Convert meshgrid to normalized space for prediction using the batch normalization parameters\n",
    "X1_normalized = (X1 - x_mean[0, 0]) / x_std[0, 0]\n",
    "X2_normalized = (X2 - x_mean[0, 1]) / x_std[0, 1]\n",
    "\n",
    "# Calculate predictions in normalized space\n",
    "Y_predicted_normalized = learned_weights[0][0] * X1_normalized + learned_weights[0][1] * X2_normalized + learned_bias[0]\n",
    "\n",
    "# Convert predictions back to denormalized space\n",
    "Y_predicted_denormalized = Y_predicted_normalized * model.y_std.cpu().numpy()[0, 0] + model.y_mean.cpu().numpy()[0, 0]\n",
    "\n",
    "# Create Plotly figure\n",
    "fig = make_subplots(rows=1, cols=1, specs=[[{'type': 'scene'}]])\n",
    "\n",
    "# Add scatter plot for data points\n",
    "scatter = go.Scatter3d(\n",
    "    x=features_sampled[:,0],\n",
    "    y=features_sampled[:,1],\n",
    "    z=targets_sampled.flatten(),\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        color='red',\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    name='Data Points',\n",
    "    hovertemplate='CQI: %{x:.2f}<br>Throughput: %{y:.2f} Mbps<br>min_prb_ratio: %{z:.2f}<extra></extra>'\n",
    ")\n",
    "fig.add_trace(scatter)\n",
    "\n",
    "# Add surface plot for the hyperplane\n",
    "surface = go.Surface(\n",
    "    x=X1, \n",
    "    y=X2, \n",
    "    z=Y_predicted_denormalized,\n",
    "    colorscale='Blues',\n",
    "    opacity=0.7,\n",
    "    showscale=False,\n",
    "    name='Predicted Hyperplane',\n",
    "    hovertemplate='CQI: %{x:.2f}<br>Throughput: %{y:.2f} Mbps<br>Predicted min_prb_ratio: %{z:.2f}<extra></extra>'\n",
    ")\n",
    "fig.add_trace(surface)\n",
    "\n",
    "# Update layout with labels and title\n",
    "fig.update_layout(\n",
    "    title='Linear Regression Hyperplane Fit (Denormalized Values)',\n",
    "    scene=dict(\n",
    "        xaxis_title='CQI',\n",
    "        yaxis_title='DRB.UEThpDL (Mbps)',\n",
    "        zaxis_title='min_prb_ratio',\n",
    "        aspectmode='auto'\n",
    "    ),\n",
    "    legend=dict(\n",
    "        y=0.99,\n",
    "        x=0.01,\n",
    "        font=dict(size=12)\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=30),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da346de9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Train/Test Splits Using the Datasets Package\n",
    "\n",
    "To better evaluate our model's performance, we can split our data into training\n",
    "and testing sets. This allows us to train on one subset and evaluate on another\n",
    "subset that the model hasn't seen, giving us a better idea of how well the model\n",
    "generalizes to new data. (The goal is not to memorize the training data, but to\n",
    "learn the underlying patterns that can be applied to new data.)\n",
    "\n",
    "We'll use the datasets package to create this split in a random but reproducible\n",
    "way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54506168",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Create the Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9313f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from our DataFrame\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert the pandas DataFrame to a datasets.Dataset object\n",
    "full_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "\"\"\"\n",
    "Split the dataset into training and testing sets (80% train, 20% test). The seed\n",
    "value is used to ensure that the split is reproducible. This means that every\n",
    "time you run the code with the same seed value, you will get the same split of\n",
    "the dataset.\n",
    "\"\"\"\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Print information about the split\n",
    "print(f\"Training set size: {len(split_dataset['train'])}\")\n",
    "print(f\"Test set size: {len(split_dataset['test'])}\")\n",
    "\n",
    "# Convert back to pandas DataFrames for easier handling\n",
    "train_df = split_dataset['train'].to_pandas()\n",
    "test_df = split_dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0082b168",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Extract Features and Target from Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981fd98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target from train and test sets\n",
    "X_train = torch.tensor(train_df[['CQI', 'DRB.UEThpDl']].values, dtype=torch.float32)\n",
    "y_train = torch.tensor(train_df['min_prb_ratio'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_test = torch.tensor(test_df[['CQI', 'DRB.UEThpDl']].values, dtype=torch.float32)\n",
    "y_test = torch.tensor(test_df['min_prb_ratio'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b5cce0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Train on the Test Set\n",
    "\n",
    "Now we'll train a new model using only the training data and evaluate it on the\n",
    "test data, which should help ensure that our model generalizes well to unseen\n",
    "data. We'll use the same model architecture as before, but we'll calculate the\n",
    "normalization parameters (mean and standard deviation) from the training data\n",
    "only. This is important because we want to avoid using any information from the\n",
    "test set during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025dfed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new model for the train/test experiment\n",
    "model_split = LinearRegressionModel()\n",
    "\n",
    "# Calculate normalization parameters from training data only\n",
    "# Important: we only use training data statistics for normalization\n",
    "model_split.y_mean = y_train.mean(dim=0, keepdim=True)\n",
    "model_split.y_std = y_train.std(dim=0, keepdim=True)\n",
    "\n",
    "# Move everything to the device\n",
    "model_split.to(device)\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model_split.parameters(), lr=0.05)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 200\n",
    "\n",
    "# Track training and testing losses\n",
    "train_losses = np.zeros(num_epochs)\n",
    "test_losses = np.zeros(num_epochs)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Training mode\n",
    "    model_split.train()\n",
    "    \n",
    "    # Forward pass with training data\n",
    "    y_train_pred = model_split(X_train)\n",
    "    train_loss = criterion(y_train_pred, (y_train - model_split.y_mean) / model_split.y_std)\n",
    "    train_losses[epoch] = train_loss.item()\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluation mode - no gradients needed\n",
    "    model_split.eval()\n",
    "    with torch.no_grad():\n",
    "        # Forward pass with test data\n",
    "        y_test_pred = model_split(X_test)\n",
    "            # Calculate test loss on normalized data\n",
    "        test_loss = criterion(\n",
    "            (y_test_pred - model_split.y_mean) / model_split.y_std, \n",
    "            (y_test - model_split.y_mean) / model_split.y_std\n",
    "        )\n",
    "        test_losses[epoch] = test_loss.item()\n",
    "    \n",
    "    if (epoch) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Normalized Train Loss: {train_loss.item():.4f}, Normalized Test Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d216d4dc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Plot Both Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdb4a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and test loss curves\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.arange(num_epochs), \n",
    "    y=train_losses, \n",
    "    mode='lines', \n",
    "    name='Training Loss'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.arange(num_epochs), \n",
    "    y=test_losses, \n",
    "    mode='lines', \n",
    "    name='Test Loss'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Loss Convergence with Train/Test Split',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss',\n",
    "    legend=dict(x=0.02, y=0.98)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e7a75",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Validate Model Performance on Test Set\n",
    "\n",
    "Now let's evaluate our model's performance on the test set in order to give us a\n",
    "better indication of how well our model will generalize to unseen data. We'll\n",
    "calculate some useful performance metrics as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c21ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "model_split.eval()\n",
    "\n",
    "# Make predictions on the test set\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model_split(X_test)\n",
    "\n",
    "# Convert to numpy for easier calculations\n",
    "y_test_np = y_test.cpu().numpy()\n",
    "y_test_pred_np = y_test_pred.cpu().numpy()\n",
    "\n",
    "# Calculate MSE and RMSE\n",
    "test_mse = np.mean((y_test_np - y_test_pred_np) ** 2)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "\"\"\"\n",
    "Calculate R^2 (coefficient of determination). The R^2 score indicates how well\n",
    "the model explains the variance in the target variable by comparing the residual\n",
    "variance (the variance of the errors in the model's predictions) to the total\n",
    "variance (the variance of the target variable without considering the model at\n",
    "all).\n",
    "\n",
    "An R^2 score of 1 indicates that the model perfectly explains the variance in\n",
    "the target variable, while an R^2 score of 0 indicates that the model does not\n",
    "explain any of the variance in the target variable. A negative R^2 score\n",
    "indicates that the model is worse than a simple mean prediction.\n",
    "\"\"\"\n",
    "y_test_mean = np.mean(y_test_np)\n",
    "ss_total = np.sum((y_test_np - y_test_mean) ** 2)\n",
    "ss_residual = np.sum((y_test_np - y_test_pred_np) ** 2)\n",
    "r_squared = 1 - (ss_residual / ss_total)\n",
    "\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"Test R^2: {r_squared:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7eaa4e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Polynomial Regression Model\n",
    "\n",
    "Let's see if we can do better with another model implementation. Polynomial\n",
    "regression is a type of regression analysis in which the relationship between\n",
    "the independent variables (input features) and the dependent variable (target\n",
    "variable) is modeled as an nth degree polynomial. This allows for more complex\n",
    "relationships between the input features and the target variable compared to\n",
    "linear regression, which assumes a linear relationship.\n",
    "\n",
    "The polynomial regression model can be implemented by transforming the input\n",
    "features into polynomial features. This is done by creating new features that\n",
    "are powers or combinations of the original features. For example, if we have two\n",
    "input features `x1` and `x2`, we can create polynomial features like `x1^2`,\n",
    "`x2^2`, `x1*x2`, etc. The transformed features are then used in the linear\n",
    "regression model. So, technically, we are still using linear regression, but the\n",
    "input features are transformed to allow for more complex relationships.\n",
    "\n",
    "In this case, we will use a polynomial regression model with degree 2. This\n",
    "means we will create polynomial features up to the second degree. The model will\n",
    "be able to learn quadratic relationships between the input features and the\n",
    "target variable. This can help improve the model's performance if the\n",
    "relationship between the input features and the target variable is not linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a78884",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Polynomial Regression Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba546f35",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the polynomial regression model\n",
    "class PolynomialRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, degree=2):\n",
    "        super(PolynomialRegressionModel, self).__init__()\n",
    "        self.degree = degree\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate number of polynomial features for 2 input features with degree\n",
    "        n. For 2 features with degree 2: x1, x2, x1^2, x1*x2, x2^2 = 5 features.\n",
    "        We subtract 1 because we start from degree 1 rather than 0.\n",
    "        \"\"\"\n",
    "        n_poly_features = int((degree + 1) * (degree + 2) / 2) - 1\n",
    "        \n",
    "        # Apply batch normalization to expanded polynomial features\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(n_poly_features)\n",
    "        \n",
    "        # Linear layer now accepts polynomial features as input\n",
    "        self.linear = torch.nn.Linear(n_poly_features, 1)\n",
    "        \n",
    "        # Register buffers to store the mean and standard deviation of the output features\n",
    "        self.register_buffer('y_mean', torch.zeros(1))\n",
    "        self.register_buffer('y_std', torch.ones(1))\n",
    "\n",
    "    def _polynomial_features(self, x):\n",
    "        \"\"\"\n",
    "        Generate polynomial features up to the specified degree.\n",
    "        For input [x1, x2], with degree=2, this generates [x1, x2, x1^2, x1*x2, x2^2]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        x1 = x[:, 0].view(-1, 1)\n",
    "        x2 = x[:, 1].view(-1, 1)\n",
    "        \n",
    "        # Start with degree 1 terms (original features)\n",
    "        poly_features = [x1, x2]\n",
    "        \n",
    "        # Add higher degree terms\n",
    "        for d in range(2, self.degree + 1):\n",
    "            for i in range(d + 1):\n",
    "                # Add term x1^(d-i) * x2^i\n",
    "                term = torch.pow(x1, d-i) * torch.pow(x2, i)\n",
    "                poly_features.append(term)\n",
    "        \n",
    "        \"\"\"\n",
    "        Concatenate all polynomial features into a single tensor. The resulting\n",
    "        tensor will have shape (batch_size, n_poly_features), where\n",
    "        n_poly_features is the number of polynomial features generated. For\n",
    "        example, if we have 2 input features and degree=2, the resulting tensor\n",
    "        will have shape (batch_size, 5).\n",
    "        \"\"\"\n",
    "        return torch.cat(poly_features, dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First transform input to polynomial features\n",
    "        x_poly = self._polynomial_features(x)\n",
    "        \n",
    "        \"\"\"\n",
    "        We need to normalize the polynomial features before passing them to the linear\n",
    "        layer.\n",
    "        \"\"\"\n",
    "        x_poly_normalized = self.batch_norm(x_poly)\n",
    "        \n",
    "        # Apply linear transformation to normalized polynomial features\n",
    "        output = self.linear(x_poly_normalized)\n",
    "        \n",
    "        # Denormalize output during inference\n",
    "        if not self.training:\n",
    "            with torch.no_grad():\n",
    "                output = output * self.y_std + self.y_mean\n",
    "                \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b5aef",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Training Polynomial Regression Model\n",
    "\n",
    "Now let's train and evaluate the polynomial regression model with degree=2 that\n",
    "we defined above. We'll use the same methodology as we did for the linear\n",
    "regression model and then compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa02fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the polynomial regression model with degree=2\n",
    "poly_model = PolynomialRegressionModel(degree=2)\n",
    "\n",
    "# Calculate normalization parameters from training data only (same as linear model)\n",
    "poly_model.y_mean = y_train.mean(dim=0, keepdim=True)\n",
    "poly_model.y_std = y_train.std(dim=0, keepdim=True)\n",
    "\n",
    "# Move model to device\n",
    "poly_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer (same as linear model)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(poly_model.parameters(), lr=0.05)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 200\n",
    "\n",
    "# Track training and testing losses\n",
    "poly_train_losses = np.zeros(num_epochs)\n",
    "poly_test_losses = np.zeros(num_epochs)\n",
    "\n",
    "# Train the polynomial model\n",
    "for epoch in range(num_epochs):\n",
    "    # Training mode\n",
    "    poly_model.train()\n",
    "    \n",
    "    # Forward pass with training data\n",
    "    y_train_pred = poly_model(X_train)\n",
    "    train_loss = criterion(y_train_pred, (y_train - poly_model.y_mean) / poly_model.y_std)\n",
    "    poly_train_losses[epoch] = train_loss.item()\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluation mode - no gradients needed\n",
    "    poly_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Forward pass with test data\n",
    "        y_test_pred = poly_model(X_test)\n",
    "        # Calculate test loss on normalized data\n",
    "        test_loss = criterion(\n",
    "            (y_test_pred - poly_model.y_mean) / poly_model.y_std, \n",
    "            (y_test - poly_model.y_mean) / poly_model.y_std\n",
    "        )\n",
    "        poly_test_losses[epoch] = test_loss.item()\n",
    "    \n",
    "    if (epoch) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Poly Train Loss: {train_loss.item():.4f}, Poly Test Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a563c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Comparing Training Loss Curves for Linear vs. Polynomial Models\n",
    "\n",
    "Let's compare the training and test loss curves for both models to see how they\n",
    "converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23859ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves for both models on the same plot\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Training Loss', 'Test Loss'))\n",
    "\n",
    "# Training loss curves\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(num_epochs), \n",
    "        y=train_losses, \n",
    "        mode='lines', \n",
    "        name='Linear Model (Train)',\n",
    "        line=dict(color='blue')\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(num_epochs), \n",
    "        y=poly_train_losses, \n",
    "        mode='lines', \n",
    "        name='Polynomial Model (Train)',\n",
    "        line=dict(color='red')\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Test loss curves\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(num_epochs), \n",
    "        y=test_losses, \n",
    "        mode='lines', \n",
    "        name='Linear Model (Test)',\n",
    "        line=dict(color='blue', dash='dash')\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(num_epochs), \n",
    "        y=poly_test_losses, \n",
    "        mode='lines', \n",
    "        name='Polynomial Model (Test)',\n",
    "        line=dict(color='red', dash='dash')\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Normalized Loss Convergence: Linear vs. Polynomial Models',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss',\n",
    "    height=400,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8578da36",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "#### Hmm... do you notice anything interesting about the loss curves above? Anything we might try to improve the training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e77a2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Evaluating the Polynomial Model\n",
    "\n",
    "Now that we've tuned things a bit, let's evaluate the polynomial model's\n",
    "performance on the test set, similar to how we evaluated the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daac573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the polynomial model on the test set\n",
    "poly_model.eval()\n",
    "\n",
    "# Make predictions on the test set\n",
    "with torch.no_grad():\n",
    "    y_test_pred_poly = poly_model(X_test)\n",
    "\n",
    "# Convert to numpy for easier calculations\n",
    "y_test_np = y_test.cpu().numpy()\n",
    "y_test_pred_poly_np = y_test_pred_poly.cpu().numpy()\n",
    "\n",
    "# Calculate MSE and RMSE for polynomial model\n",
    "poly_test_mse = np.mean((y_test_np - y_test_pred_poly_np) ** 2)\n",
    "poly_test_rmse = np.sqrt(poly_test_mse)\n",
    "\n",
    "# Calculate R^2 for polynomial model\n",
    "y_test_mean = np.mean(y_test_np)\n",
    "ss_total = np.sum((y_test_np - y_test_mean) ** 2)\n",
    "ss_residual_poly = np.sum((y_test_np - y_test_pred_poly_np) ** 2)\n",
    "poly_r_squared = 1 - (ss_residual_poly / ss_total)\n",
    "\n",
    "print(f\"Polynomial Model - Test MSE: {poly_test_mse:.4f}\")\n",
    "print(f\"Polynomial Model - Test RMSE: {poly_test_rmse:.4f}\")\n",
    "print(f\"Polynomial Model - Test R^2: {poly_r_squared:.4f}\")\n",
    "\n",
    "# Get linear model predictions for comparison\n",
    "model_split.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred_linear = model_split(X_test)\n",
    "\n",
    "y_test_pred_linear_np = y_test_pred_linear.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7aa60",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Comparing the Two Models\n",
    "\n",
    "Let's compare the performance of the linear and polynomial regression models in\n",
    "terms of the metrics we've been recording."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc1187",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison table of metrics\n",
    "metrics_comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Polynomial Regression (degree=2)'],\n",
    "    'MSE': [test_mse, poly_test_mse],\n",
    "    'RMSE': [test_rmse, poly_test_rmse],\n",
    "    'R^2': [r_squared, poly_r_squared]\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(metrics_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c911e1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Visualizing the Model Fits\n",
    "\n",
    "Let's also create a side-by-side 3D visualization of both models to compare\n",
    "their fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e367467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a meshgrid for visualization\n",
    "x1_range = np.linspace(X_test.cpu().numpy()[:,0].min(), X_test.cpu().numpy()[:,0].max(), 20)\n",
    "x2_range = np.linspace(X_test.cpu().numpy()[:,1].min(), X_test.cpu().numpy()[:,1].max(), 20)\n",
    "X1, X2 = np.meshgrid(x1_range, x2_range)\n",
    "\n",
    "# Create tensor grid points for predictions\n",
    "grid_points = []\n",
    "for i in range(X1.shape[0]):\n",
    "    for j in range(X1.shape[1]):\n",
    "        grid_points.append([X1[i,j], X2[i,j]])\n",
    "grid_tensor = torch.tensor(grid_points, dtype=torch.float32).to(device)\n",
    "\n",
    "# Get predictions for grid points\n",
    "with torch.no_grad():\n",
    "    grid_preds = poly_model(grid_tensor).cpu().numpy()\n",
    "\n",
    "# Reshape predictions to match grid\n",
    "Z_poly = grid_preds.reshape(X1.shape)\n",
    "\n",
    "# Sample test points for visualization (to reduce clutter)\n",
    "sample_indices = np.random.choice(X_test.shape[0], size=min(100, X_test.shape[0]), replace=False)\n",
    "X_test_sample = X_test[sample_indices].cpu().numpy()\n",
    "y_test_sample = y_test[sample_indices].cpu().numpy()\n",
    "\n",
    "# Create a grid tensor from the meshgrid points for the linear model predictions\n",
    "grid_points = np.column_stack([X1.flatten(), X2.flatten()])\n",
    "grid_tensor = torch.tensor(grid_points, dtype=torch.float32).to(device)\n",
    "\n",
    "# Get linear model predictions for the same grid\n",
    "model_split.eval()\n",
    "with torch.no_grad():\n",
    "    grid_preds_linear = model_split(grid_tensor).cpu().numpy()\n",
    "\n",
    "# Reshape predictions to match grid\n",
    "Z_linear = grid_preds_linear.reshape(X1.shape)\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{'type': 'scene'}, {'type': 'scene'}]],\n",
    "    subplot_titles=('Linear Model', 'Polynomial Model (Degree=2)')\n",
    ")\n",
    "\n",
    "# Linear model surface\n",
    "fig.add_trace(\n",
    "    go.Surface(\n",
    "        x=X1, \n",
    "        y=X2, \n",
    "        z=Z_linear,\n",
    "        colorscale='Blues',\n",
    "        opacity=0.7,\n",
    "        name='Linear Model',\n",
    "        showscale=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Test data points for linear model subplot\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=X_test_sample[:,0],\n",
    "        y=X_test_sample[:,1],\n",
    "        z=y_test_sample.flatten(),\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=3,\n",
    "            color='black',\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        name='Test Data',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Polynomial model surface\n",
    "fig.add_trace(\n",
    "    go.Surface(\n",
    "        x=X1, \n",
    "        y=X2, \n",
    "        z=Z_poly,\n",
    "        colorscale='Reds',\n",
    "        opacity=0.7,\n",
    "        name='Polynomial Model',\n",
    "        showscale=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Test data points for polynomial model subplot\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=X_test_sample[:,0],\n",
    "        y=X_test_sample[:,1],\n",
    "        z=y_test_sample.flatten(),\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=3,\n",
    "            color='black',\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        name='Test Data'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update subplot scene properties\n",
    "for i in [1, 2]:\n",
    "    fig.update_scenes(\n",
    "        xaxis_title='CQI',\n",
    "        yaxis_title='DRB.UEThpDl',\n",
    "        zaxis_title='min_prb_ratio',\n",
    "        aspectmode='auto',\n",
    "        row=1, col=i\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Model Comparison: Linear vs. Polynomial Surface Fits',\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    margin=dict(l=0, r=0, b=0, t=50)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8c115",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Exporting the Model to Hugging Face and Verifying We can Load it Back\n",
    "\n",
    "Let's pretend that our work is done here and save a trained model to HuggingFace\n",
    "so we can use it later. We will first export the model to ONNX format.\n",
    "\n",
    "ONNX (Open Neural Network Exchange) is an open format for representing machine\n",
    "learning models. It allows models to be transferred between different frameworks\n",
    "and platforms, making it easier to deploy models in various environments. ONNX\n",
    "provides a standardized way to represent models, enabling interoperability\n",
    "between different deep learning frameworks such as PyTorch, TensorFlow, and\n",
    "others.\n",
    "\n",
    "[ONNX documentation](https://onnx.ai/onnx/intro/)\n",
    "\n",
    "We're using ONNX to export our trained PyTorch model so our inference server can\n",
    "simply use the ONNX runtime to load the model and run inference on it without\n",
    "any additional dependencies or libraries, and without needing to worry about\n",
    "importing the classes and methods you designed for your models.\n",
    "\n",
    "We'll upload the polynomial regression model as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18db8bf1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Save the Polynomial Regression Model to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93cfe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the polynomial regression model to ONNX and upload to Hugging Face\n",
    "\n",
    "poly_model_name = f\"polynomial_regression_model_{username}\"\n",
    "poly_model_version = \"0.0.1\"  # alpha version\n",
    "\n",
    "\"\"\"\n",
    "We'll save some metadata about the model that may be useful.\n",
    "\"\"\"\n",
    "poly_metadata_props = {\n",
    "    \"version\": poly_model_version,\n",
    "    \"training_date\": datetime.datetime.now().isoformat(),\n",
    "    \"framework\": f\"PyTorch {torch.__version__}\",\n",
    "    \"dataset\": f\"{dataset_name}\",\n",
    "    \"metrics\": json.dumps({\n",
    "        \"mse\": f\"{poly_test_mse}\",\n",
    "        \"rmse\": f\"{poly_test_rmse}\",\n",
    "        \"r2\": f\"{poly_r_squared}\"\n",
    "    }),\n",
    "    \"description\": f\"Polynomial regression model for min PRB prediction based on CQI and DRB.UEThpDl.\",\n",
    "    \"input_features\": json.dumps([\"CQI\", \"DRB.UEThpDl\"]),\n",
    "    \"output_features\": json.dumps([\"min_prb_ratio\"]),\n",
    "    \"polynomial_degree\": poly_model.degree,\n",
    "    \"model_type\": \"polynomial_regression\"\n",
    "}\n",
    "\n",
    "# Create temp directory\n",
    "poly_temp_dir = tempfile.mkdtemp()\n",
    "poly_model_path = os.path.join(poly_temp_dir, f\"{poly_model_name}_v{poly_model_version}.onnx\")\n",
    "\n",
    "# Export the model to ONNX\n",
    "dummy_input = torch.randn(1, 2)  # Example input\n",
    "torch.onnx.export(\n",
    "    poly_model, \n",
    "    dummy_input, \n",
    "    poly_model_path, \n",
    "    verbose=True, \n",
    "    input_names=[\"input\"], \n",
    "    output_names=[\"output\"], \n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    ")\n",
    "\n",
    "# Create repository for polynomial model\n",
    "poly_model_repo = f\"cyberpowder/{poly_model_name}_v{poly_model_version}\"\n",
    "\n",
    "# Create metadata JSON file\n",
    "poly_metadata_path = os.path.join(poly_temp_dir, f\"{poly_model_name}_v{poly_model_version}_metadata.json\")\n",
    "with open(poly_metadata_path, 'w') as f:\n",
    "    json.dump(poly_metadata_props, f, indent=2)\n",
    "\n",
    "# Create or ensure repository exists\n",
    "try:\n",
    "    hf_api.create_repo(poly_model_repo, private=True, token=token)\n",
    "    print(f\"Created new repository: {poly_model_repo}\")\n",
    "except Exception as e:\n",
    "    print(f\"Repository may already exist or error creating it: {e}\")\n",
    "\n",
    "# Upload ONNX model to Hugging Face\n",
    "print(f\"Uploading polynomial ONNX model to Hugging Face: {poly_model_repo}\")\n",
    "hf_api.upload_file(\n",
    "    path_or_fileobj=poly_model_path,\n",
    "    repo_id=poly_model_repo,\n",
    "    path_in_repo=f\"{poly_model_name}_v{poly_model_version}.onnx\",\n",
    "    token=token\n",
    ")\n",
    "\n",
    "# Upload metadata to Hugging Face\n",
    "print(f\"Uploading polynomial model metadata to Hugging Face: {poly_model_repo}\")\n",
    "hf_api.upload_file(\n",
    "    path_or_fileobj=poly_metadata_path,\n",
    "    repo_id=poly_model_repo,\n",
    "    path_in_repo=f\"{poly_model_name}_v{poly_model_version}_metadata.json\",\n",
    "    token=token\n",
    ")\n",
    "\n",
    "print(f\"Polynomial model and metadata successfully uploaded to Hugging Face: {poly_model_repo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48ef596",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Verify ONNX Model Download and Inference\n",
    "\n",
    "Now let's make sure that we can load the model back from HuggingFace and run\n",
    "inference using the ONNX runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in the repo to confirm upload was successful\n",
    "print(f\"Files in repository {poly_model_repo}:\")\n",
    "model_files = hf_api.list_repo_files(poly_model_repo, token=token)\n",
    "for file in model_files:\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# Create temporary directory for downloaded files\n",
    "poly_download_dir = tempfile.mkdtemp()\n",
    "\n",
    "try:\n",
    "    # Download model from Hugging Face\n",
    "    print(f\"\\nDownloading polynomial model from Hugging Face...\")\n",
    "    poly_model_path = hf_hub_download(\n",
    "        repo_id=poly_model_repo,\n",
    "        filename=f\"{poly_model_name}_v{poly_model_version}.onnx\",\n",
    "        token=token,\n",
    "        local_dir=poly_download_dir\n",
    "    )\n",
    "    \n",
    "    # Download metadata\n",
    "    print(f\"Downloading polynomial model metadata from Hugging Face...\")\n",
    "    poly_metadata_path = hf_hub_download(\n",
    "        repo_id=poly_model_repo,\n",
    "        filename=f\"{poly_model_name}_v{poly_model_version}_metadata.json\",\n",
    "        token=token,\n",
    "        local_dir=poly_download_dir\n",
    "    )\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(poly_metadata_path, 'r') as f:\n",
    "        poly_metadata = json.load(f)\n",
    "    \n",
    "    print(f\"\\nPolynomial model metadata:\")\n",
    "    print(f\"  Version: {poly_metadata.get('version')}\")\n",
    "    print(f\"  Polynomial degree: {poly_metadata.get('polynomial_degree')}\")\n",
    "    print(f\"  Description: {poly_metadata.get('description')}\")\n",
    "    print(f\"  Framework: {poly_metadata.get('framework')}\")\n",
    "    print(f\"  Metrics: {poly_metadata.get('metrics')}\")\n",
    "    \n",
    "    # Load model with ONNX Runtime\n",
    "    print(f\"\\nLoading polynomial model for inference...\")\n",
    "    poly_session = ort.InferenceSession(poly_model_path)\n",
    "    \n",
    "    # Some sample data for inference\n",
    "    # Multiplying the DRB.UEThpDL here by 1000 to convert Mbps to Kbps (model expects Kbps)\n",
    "    sample_inputs = [\n",
    "        [6.0, 1.0 * 1000.0],    # Low CQI, low throughput\n",
    "        [10.0, 100.0 * 1000.0],  # Medium CQI, medium throughput\n",
    "        [15.0, 300.0 * 1000.0]   # High CQI, high throughput\n",
    "    ]\n",
    "    \n",
    "    input_tensor = np.array(sample_inputs, dtype=np.float32)\n",
    "    \n",
    "    # Run inference with polynomial model\n",
    "    print(f\"\\nRunning inference with sample data...\")\n",
    "    poly_outputs = poly_session.run(None, {\"input\": input_tensor})\n",
    "    \n",
    "    # Print results as a table\n",
    "    print(\"\\nPrediction Results:\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "    print(\"   CQI   | Throughput (Mbps) | Predicted min_prb_ratio\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "    for i, sample in enumerate(sample_inputs):\n",
    "        print(f\"  {sample[0]:5.1f}  |      {sample[1]/1000.0:7.1f}     |        {poly_outputs[0][i][0]:7.2f}\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during download or inference: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9314debd",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Discussion\n",
    "\n",
    "Notice any issues with the predictions? There is a significant issue we've\n",
    "ignored up until now and I chose the sample inputs to illustrate it. Recall\n",
    "that, for our application, the priority slice will never have less than 50% of\n",
    "the resources...\n",
    "\n",
    "Also, the CQI feature and the min_prb_ratio target have a certain characteristic\n",
    "that doesn't mesh well with a regression model...\n",
    "\n",
    "Maybe have a look at the dataframe from your dataset again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffc403d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Examine the Dataset Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5974cfa",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "After some discussion, we will add more detail to the IH9 O-RAN + ML assignment\n",
    "on canvas, and you can start working on it for the remainder of today's session,\n",
    "and continue as HW."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
