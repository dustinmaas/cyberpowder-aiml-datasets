{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c8b5cd",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# O-RAN Dataset Analysis and Processing\n",
    "\n",
    "1. Brief HuggingFace intro\n",
    "  1. Logging in to HuggingFace\n",
    "  2. Creating and uploading datasets to HuggingFace\n",
    "  3. Downloading datasets from HuggingFace\n",
    "2. Analyzing the O-RAN slicing dataset\n",
    "  1. Loading the dataset\n",
    "  2. Some initial data processing\n",
    "  3. Visualizing the dataset\n",
    "  4. Brainstorm and apply further data processing\n",
    "  5. Uploading the processed dataset to HuggingFace\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "- Read Section VI (AI/ML Workflows) of the [NEU ORAN paper](https://utah.instructure.com/courses/1045795/files/170447527?wrap=1)\n",
    "- Join the [HuggingFace CyberPowder organization](https://huggingface.co/cyberpowder)\n",
    "  - Instructions [here](https://utah.instructure.com/courses/1045795/assignments/15915757)\n",
    "\n",
    "## Note: Don't just run the whole notebook. There are some cells that will require interaction. Read the comments and understand what each cell does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554305f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (various other required packages are already available in the colab environment)\n",
    "!uv -q pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c76b8b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import datetime\n",
    "\n",
    "import datasets\n",
    "import huggingface_hub as hf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c290c90",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 1. Brief HuggingFace intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a13b3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Logging in to HuggingFace\n",
    "\n",
    "Using the HuggingFace API requires and access token. We will walk through this\n",
    "now, but there are some instructions\n",
    "[here](https://app.excalidraw.com/s/8g7kivZ39v0/8x63aN6Ps5B?element=Z4ECPgkcwhcUUXJmXPnyA)\n",
    "as well.\n",
    "\n",
    "Running the cell below will prompt you to log in to HuggingFace using the\n",
    "generated access token, so let's go generate one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf0f53",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Check that you're logged in (ignore the warning about adding the token as a\n",
    "Colab secret)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a6a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = hf.whoami()['name']\n",
    "print(f\"Logged in as {username}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0577accf",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "From here on out, calls to the HuggingFace API should be automatically\n",
    "authenticated with your access token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f411717",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Creating and Uploading Datasets to Hugging Face\n",
    "\n",
    "Let's create some random data to use as a dataset. We'll use numpy to generate\n",
    "random features and targets and throw them into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1605dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_features = np.random.rand(100, 2)  # 100 samples, 2 features\n",
    "random_targets = np.random.rand(100, 1)  # 100 samples, 1 target\n",
    "df = pd.DataFrame(random_features, columns=[\"feature1\", \"feature2\"])\n",
    "df[\"target\"] = random_targets\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852dd7f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now we'll use the datasets library to create a dataset from the pandas DataFrame.\n",
    "You could also create a dataset from a csv file, json file, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e8fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070cf149",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now let's create a dataset repository on HuggingFace to store the dataset we just created.\n",
    "We'll use the current date and time to make the dataset name unique.  We'll also\n",
    "make the dataset private so that only you can access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_repo_name = f\"{username}/dummy-datasets\"\n",
    "dummy_dataset_name = f\"dummy-dataset-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "# (this will raise an error if the repository already exists, but we'll ignore that for now)\n",
    "hf_api = hf.HfApi()\n",
    "try:\n",
    "    hf_api.create_repo(dummy_repo_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating repository: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc3a44",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now we'll push the dataset to the repository we just created.  If the dataset\n",
    "already exists, and there are no changes, HuggingFace will not create a new\n",
    "version/commit. We'll make the dataset private so that only you can access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1376067",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(dummy_repo_name, private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f7db2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Downloading the dataset\n",
    "\n",
    "Now let's make sure we can download the dataset we just uploaded.  We'll first\n",
    "use the HuggingFace API to list the datasets in the repository. You should see\n",
    "the dataset we just uploaded in the list of datasets. It may be the only dataset\n",
    "in the list if you haven't uploaded any others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_datasets = hf_api.list_datasets(author=username)\n",
    "for ds in my_datasets:\n",
    "    print(f\"Dataset: {ds.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbd005",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's download the dataset we just uploaded using the datasets package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383106a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(dummy_repo_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf1c6c6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "We can now access the dataset as a dictionary. Since we didn't specify a\n",
    "train/test split when we uploaded the dataset, all of the data is in the \"train\"\n",
    "key in the dataset dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset['train'].to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107545e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "## 2. Analyzing the O-RAN slicing dataset\n",
    "\n",
    "Now, let's move on to interacting with the O-RAN slicing dataset that we'll be\n",
    "processing today for use in next Friday's session, where you will each create,\n",
    "train, and validate a model using PyTorch.\n",
    "\n",
    "This dataset consists of network metrics collected during a long run of the\n",
    "example experiment we looked at earlier. The data were pulled from the data lake\n",
    "and pushed to HuggingFace. You can find the dataset at:\n",
    "https://huggingface.co/datasets/cyberpowder/cyberpowder-network-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8fea9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Loading the slicing dataset\n",
    "\n",
    "We'll use the datasets library to again load the dataset using the appropriate\n",
    "repo name and the dataset configuration name (default) used when it was uploaded\n",
    "to HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_repo_name = \"cyberpowder/cyberpowder-network-metrics\"\n",
    "oran_slicing_dataset = datasets.load_dataset(cp_repo_name, \"default\")\n",
    "oran_slicing_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d96761",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Some initial data processing (part of step ii. as described in the paper)\n",
    "\n",
    "The first thing to be aware of is that we don't need all of the data that came\n",
    "out of the data lake for this experiment.\n",
    "\n",
    "First, we'll turn the dataset into a pandas DataFrame so that we can use the\n",
    "pandas package to process the data. Again, this dataset was created without a\n",
    "train/test split, so all of the data is in the \"train\" key in the dataset\n",
    "dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab6861",
   "metadata": {},
   "outputs": [],
   "source": [
    "oran_slicing_df = oran_slicing_dataset['train'].to_pandas()\n",
    "oran_slicing_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0abddf",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Note that we have KPIs for two UEs as expected, but our simple application (and\n",
    "future model) only cares about performance guarantees for the emergency responder\n",
    "UE, which holds `ue_id` 1 in this dataset. The second UE, which happens to hold\n",
    "`ue_id` 3 in the dataset (for reasons that aren't important here), is the\n",
    "consumer UE.\n",
    "\n",
    "Before we start trying to further understand the data, let's remove the\n",
    "consumer UE from the dataset. We can do this by filtering the DataFrame to\n",
    "only include rows for `ue_id` 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1766b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "oran_slicing_df_ue1 = oran_slicing_df[oran_slicing_df['ue_id'] == 1]\n",
    "oran_slicing_df_ue1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e4ed5b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "It might be useful in the future to have direct access to this filtered dataset,\n",
    "so let's create a new dataset repo under our personal HuggingFace account, and\n",
    "upload the filtered dataset to that repo. We'll use the configuration name\n",
    "\"emergency-responder-data\" to indicate that this dataset only contains data for the\n",
    "emergency responder UE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c932f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset from the filtered DataFrame\n",
    "my_dataset_repo_name = f\"{username}/cyberpowder-network-metrics\"\n",
    "my_dataset_config_name = \"emergency-responder-data\"\n",
    "\n",
    "# (this will raise an error if the repository already exists, but we'll ignore\n",
    "# that for now, since it will probably only happen to me)\n",
    "try:\n",
    "    hf_api.create_repo(my_dataset_repo_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating repository: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the filtered dataset to the new repository\n",
    "oran_slicing_dataset_ue1 = datasets.Dataset.from_pandas(oran_slicing_df_ue1)\n",
    "oran_slicing_dataset_ue1.push_to_hub(\n",
    "    my_dataset_repo_name,\n",
    "    config_name=my_dataset_config_name,\n",
    "    private=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3e23c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Visualizing the dataset\n",
    "\n",
    "Now that we have the emergency responder dataset, let's take a closer look at\n",
    "it. We'll use the plotly package to create some interactive plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da514c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's make a copy of our DataFrame with a short name to make the rest of the\n",
    "code less verbose. We'll also convert the timestamp column to a datetime object\n",
    "so that we can use it as the x-axis in our plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a92de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = oran_slicing_df_ue1.copy()\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a99d4dc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now let's look at the key KPIs and experimental parameters in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e6c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Add each metric as a separate trace\n",
    "fig.add_trace(go.Scatter(x=df['timestamp'], y=df['atten'], mode='lines', name='atten'))\n",
    "fig.add_trace(go.Scatter(x=df['timestamp'], y=df['CQI'], mode='lines', name='CQI'))\n",
    "fig.add_trace(go.Scatter(x=df['timestamp'], y=df['RSRP'], mode='lines', name='RSRP'))\n",
    "fig.add_trace(go.Scatter(x=df['timestamp'], y=df['DRB.UEThpDl'] / 1000.0, mode='lines', name='DRB.UEThpDl (Mbps)'))\n",
    "fig.add_trace(go.Scatter(x=df['timestamp'], y=df['min_prb_ratio'], mode='lines', name='min_prb_ratio'))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Time Series of Network Metrics',\n",
    "    xaxis_title='Timestamp',\n",
    "    yaxis_title='Value',\n",
    "    legend_title='KPIs and Parameters',\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "# Add range slider\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        rangeslider=dict(visible=True),\n",
    "        type=\"date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061064d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Go ahead and play around with the figure a bit. Do you notice anything interesting?\n",
    "\n",
    "There are several unsurprising things about the data, but some of it looks a bit strange. How? Why?\n",
    "\n",
    "We'll move on to the next plot after some discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876e517",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "OK. Let's look at another view of the data. This time we'll make separate\n",
    "scatter plots showing DRB.UEThpDl vs. CQI for different min_prb_ratio values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scatter_for_prb(df, prb_value):\n",
    "    df_filtered = df[df['min_prb_ratio'] == prb_value]\n",
    "    return go.Scatter(\n",
    "        x=df_filtered['CQI'],\n",
    "        y=df_filtered['DRB.UEThpDl'] / 1000.0,  # Convert to Mbps\n",
    "        mode='markers',\n",
    "        name=f'min_prb_ratio = {prb_value}',\n",
    "        marker=dict(\n",
    "            size=8,\n",
    "            opacity=0.7,\n",
    "        ),\n",
    "        hovertemplate='CQI: %{x}<br>Throughput: %{y:.2f} Mbps<extra></extra>'\n",
    "    )\n",
    "\n",
    "# Get unique min_prb_ratio values\n",
    "unique_prb_values = sorted(df['min_prb_ratio'].unique())\n",
    "\n",
    "# We don't need plots for every min_prb_ratio value, so let's just take every fifth value\n",
    "unique_prb_values = unique_prb_values[::5]\n",
    "\n",
    "# Create subplot grid with one subplot per min_prb_ratio value\n",
    "fig = make_subplots(\n",
    "    rows=1, \n",
    "    cols=len(unique_prb_values),\n",
    "    subplot_titles=[f'min_prb_ratio = {val}' for val in unique_prb_values],\n",
    "    shared_yaxes=True\n",
    ")\n",
    "\n",
    "# Add a scatter trace for each min_prb_ratio value\n",
    "for i, prb_value in enumerate(unique_prb_values):\n",
    "    fig.add_trace(\n",
    "        make_scatter_for_prb(df, prb_value),\n",
    "        row=1, \n",
    "        col=i+1\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Throughput vs. CQI by min_prb_ratio',\n",
    "    height=500,\n",
    "    width=200 * len(unique_prb_values),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Update axes labels\n",
    "for i in range(len(unique_prb_values)):\n",
    "    fig.update_xaxes(title_text=\"CQI\", row=1, col=i+1)\n",
    "    if i == 0:  # Only add y-axis title to the first subplot\n",
    "        fig.update_yaxes(title_text=\"Throughput (Mbps)\", row=1, col=i+1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6383f771",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Now take some time to examine this set of figures. Any new insights?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2a7ea0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Brainstorm and apply further data processing\n",
    "\n",
    "If you are a data science or ML expert (or even a budding one), how might you\n",
    "further process the data to make training a model more effective? What kind of\n",
    "model do you think you might use? \n",
    "\n",
    "Remember, the goal for our emergency responder application is to predict the\n",
    "required min_prb_ratio to meet a given DRB.UEThpDl throughput requirement for a\n",
    "given CQI value.\n",
    "\n",
    "If you are used to using other tools for understanding and processing data,\n",
    "there's a good chance that the Colab environment already includes them. If not,\n",
    "you can use the !pip command to install them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf08c4c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Continue with your own data processing. We can discuss as you work.\n",
    "\n",
    "The HW assignment for this session is to:\n",
    "\n",
    "1. Process the dataset in ways that will make training a model more effective\n",
    "2. Upload your processed dataset to HuggingFace when done (upload under your\n",
    "   user account, not the CyberPowder Org, and keep it private for now)\n",
    "3. Generate both of the plotly figures we created above using your processed\n",
    "   dataset and save them using the \"Download Plot as PNG\" option in the plotly\n",
    "   figure menu\n",
    "4. Save the code snippets you used for your data processing\n",
    "5. Generate a brief report that includes:\n",
    "  - The code snippets you used to further process the dataset \n",
    "  - The plot images you saved \n",
    "  - Your reasoning behind the data processing steps you took\n",
    "\n",
    "You can use these processd datasets in the next CyberPowder session to train a\n",
    "model. The complete homework description will also be posted on the course\n",
    "website later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
